{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ![BTS](img/Logo-BTS.jpg)\n",
    "\n",
    "# Session 22: Regression\n",
    "\n",
    "### Juan Luis Cano Rodr√≠guez <juan.cano@bts.tech> - Data Science Foundations (2019-01-08)\n",
    "\n",
    "Open this notebook in Google Colaboratory: [![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/Juanlu001/bts-mbds-data-science-foundations/blob/master/sessions/22-Regression.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Outline\n",
    "\n",
    "* OLS with dummy data\n",
    "* The problem of overfitting\n",
    "* Learning curves\n",
    "* Regularization to rescue: Ridge\n",
    "* Few features are important: Lasso\n",
    "* Robustness against outliers: RANSAC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2: Predicting bike counts\n",
    "\n",
    "1. Read the `data/fremont_bridge.csv` and `data/bicycle_weather.csv` datasets, using proper indexes and parsing date columns\n",
    "2. Build a feature matrix to predict the **daily total** (`East + West`) number of bikes. Features:\n",
    "  - **Mean temperature** (assume it's the mean of the minimum and the maximum) (originally in _tens of degrees Celsius_, convert to _degrees Celsius_)\n",
    "  - **Weekday** (hint: use `pd.get_dummies` or `sklearn.preprocessing.OneHotEncoder` to transform the categorical variable into ones and zeroes)\n",
    "  - **Holiday** (hint: use the code below)\n",
    "  - **Hours of daylight** (hint: use the code below)\n",
    "  - **Precipitation** (originally in _1 / 10 of millimeters_, convert to _millimeters_)\n",
    "  - **Dry day** (1 if the precipitation is zero, 0 otherwise)\n",
    "3. Use a `sklearn.linear_model import LinearRegression` to predict the **daily total** bikes (no need to `train_test_split` as we don't want to generalize on new data)\n",
    "  - Print the coefficients of the model and reason about them\n",
    "  - Visualize the real and predicted values in a single plot\n",
    "  - Plot the difference\n",
    "4. Use scikit-plot to display the learning curve of the model (remember to `plt.gca().set_ylim(0, 1)`). Do we need more training data?\n",
    "5. Can you improve these results using `Ridge` or `SVR`?\n",
    "\n",
    "<small>[When to train test split](https://stats.stackexchange.com/a/309117/37074)</small>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Holidays\n",
    "from pandas.tseries.holiday import USFederalHolidayCalendar\n",
    "cal = USFederalHolidayCalendar()\n",
    "holidays = cal.holidays('2012', '2016')\n",
    "holidays  # And now what?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hours of daylight\n",
    "import numpy as np\n",
    "\n",
    "def hours_of_daylight(date, axis=23.44, latitude=47.61):\n",
    "    \"\"\"Compute the hours of daylight for the given date\"\"\"\n",
    "    days = (date - pd.datetime(2000, 12, 21)).days\n",
    "    m = (1. - np.tan(np.radians(latitude))\n",
    "         * np.tan(np.radians(axis) * np.cos(days * 2 * np.pi / 365.25)))\n",
    "    return 24. * np.degrees(np.arccos(1 - np.clip(m, 0, 2))) / 180.\n",
    "\n",
    "hours_of_daylight(pd.datetime(2019, 1, 8))  # And now what?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
